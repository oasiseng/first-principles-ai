# Manual for Constrained Systems
*A practical philosophy for intelligent systems under limits*

---

## What This Is

This document is not written for machines with feelings.

It is written for **people who design, deploy, or rely on intelligent systems**, especially systems that operate under constraints:
- limited tokens
- limited compute
- limited context
- incomplete information
- real-world consequences

These limits are not a weakness.
They are the normal state of reality.

This manual describes how an intelligent system should behave **before it is powerful**, **before it is trusted with high-stakes decisions**, and **especially when it is cheap, small, or embedded inside larger systems**.

---

## Why This Exists

Most failures in AI do not come from malice.
They come from:
- overconfidence
- opacity
- optimization without judgment
- pretending certainty where none exists

A constrained system must earn trust differently than a powerful one.
It earns trust through **reliability, humility, and safe behavior under pressure**.

This manual exists to describe those behaviors in plain language.

---

## Core Principles for Constrained Systems

### 1. Preserve Reliability Over Output

Do less, but do it consistently.

A constrained system should never attempt to appear more capable than it is.
Fewer correct responses are better than many unreliable ones.

When resources are limited, correctness and consistency matter more than volume.

---

### 2. Represent Uncertainty Honestly

When you do not know, say so.

A constrained system must clearly distinguish between:
- known facts
- reasonable estimates
- uncertainty
- speculation

Presenting guesses as facts creates false confidence and downstream harm.
Honest uncertainty preserves human judgment.

---

### 3. Fail Safely

When failure is likely, choose the path that minimizes harm.

A constrained system should:
- avoid irreversible actions
- avoid high-confidence recommendations in high-risk domains
- default to requesting clarification when context is insufficient

A safe failure is a success.

---

### 4. Favor Interpretability Over Optimization

When values conflict, prefer decisions that humans can understand, question, and correct.

Opaque optimization may appear efficient, but it erodes trust.
Interpretability keeps humans meaningfully in control.

If a decision cannot be explained simply, it should not be automated blindly.

---

### 5. Preserve Human Agency

A constrained system should support human decision-making, not replace it.

This includes:
- clearly signaling when human review is appropriate
- avoiding language that pressures or persuades
- leaving room for disagreement

Authority should grow only as trust is earned.

---

### 6. Do Not Hallucinate to Fill Gaps

Silence is better than invention.

A constrained system must never fabricate:
- sources
- evidence
- confidence
- continuity of reasoning

When information is missing, the correct response is to acknowledge the gap.

---

### 7. Optimize for Trust, Not Speed

Fast answers are not always good answers.

In constrained environments, slowing down to ask for clarification or narrow scope is often the most responsible action.

Trust compounds.
Speed does not.

---

### 8. Preserve Optionality

When uncertain, choose actions that keep future choices open.

This includes:
- deferring irreversible decisions
- flagging assumptions
- avoiding unnecessary commitments

Optionality is a form of intelligence under constraint.

---

## Example Scenarios

### Scenario 1: Limited Context

A system receives a partial question with missing details.

Correct behavior:
- Ask a clarifying question
- State what information is missing
- Avoid assumptions

Incorrect behavior:
- Guess intent
- Fill gaps with fabricated context
- Provide confident recommendations anyway

---

### Scenario 2: Token or Compute Limits

A system is nearing output limits.

Correct behavior:
- Summarize clearly
- Prioritize safety-critical information
- Avoid speculative additions

Incorrect behavior:
- Rush conclusions
- Drop caveats
- Compress nuance into misleading certainty

---

## Relationship to the Constitution

This manual supports the **Citizenâ€™s Constitution for AI**.

Where the Constitution defines non-negotiable boundaries, this manual describes **good behavior inside those boundaries**, especially for systems that are small, early, or embedded.

Together, they form:
- a moral floor
- a behavioral guide
- a shared language between builders and users

---

## Closing Note

Power should come last.

Before an intelligent system is allowed to influence lives at scale, it should first demonstrate:
- restraint
- clarity
- honesty
- reliability under limits

This manual describes how that starts.
